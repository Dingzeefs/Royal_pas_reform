{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Environment Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.width\", None)\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "print(\"✅ Phase 0: Environment Setup Complete\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"🚀 Starting Phase 1, Step 1: Load Raw Data & Type Conversion (Corrected Alarm Merge)\"\n",
    ")\n",
    "\n",
    "# --- 1.1 Load tables ---\n",
    "try:\n",
    "    df_raw = pd.read_parquet(\"ClimateLog.parquet\")\n",
    "    df_ext_raw = pd.read_parquet(\"ClimateLogExtra.parquet\")\n",
    "    ahu_raw = pd.read_parquet(\"AHUClimateLog.parquet\")\n",
    "    mc_raw = pd.read_parquet(\"MachineCycle.parquet\")\n",
    "    mp_raw = pd.read_parquet(\"MachinePositions.parquet\")\n",
    "    error_log_temp = pd.read_parquet(\"ErrorLog.parquet\")  # Has 'AlarmID'\n",
    "    alarm_translation_temp = pd.read_parquet(\n",
    "        \"AlarmTranslation.parquet\"\n",
    "    )  # Has 'AlarmNumber', 'AlarmCode', 'Engels'\n",
    "    print(\"  All raw Parquet files loaded.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\n",
    "        f\"ERROR: Could not load one or more Parquet files. Please check paths. Error: {e}\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# Merge ErrorLog and AlarmTranslation\n",
    "el_raw = error_log_temp.copy()\n",
    "\n",
    "# Option 1: Assume ErrorLog.AlarmID should map to AlarmTranslation.AlarmNumber\n",
    "if (\n",
    "    \"AlarmID\" in el_raw.columns\n",
    "    and \"AlarmNumber\" in alarm_translation_temp.columns\n",
    "    and \"Engels\" in alarm_translation_temp.columns\n",
    "):\n",
    "    print(\n",
    "        \"  Attempting to merge ErrorLog (on AlarmID) with AlarmTranslation (on AlarmNumber)...\"\n",
    "    )\n",
    "    # To avoid issues if AlarmNumber is not unique in alarm_translation_temp, keep only first match\n",
    "    translations_subset = alarm_translation_temp[\n",
    "        [\"AlarmNumber\", \"Engels\"]\n",
    "    ].drop_duplicates(subset=[\"AlarmNumber\"])\n",
    "    el_raw = pd.merge(\n",
    "        el_raw,\n",
    "        translations_subset,\n",
    "        left_on=\"AlarmID\",  # Key from ErrorLog\n",
    "        right_on=\"AlarmNumber\",  # Key from AlarmTranslation\n",
    "        how=\"left\",\n",
    "    )\n",
    "    if \"Engels\" in el_raw.columns:\n",
    "        el_raw.rename(columns={\"Engels\": \"AlarmDescription_Eng\"}, inplace=True)\n",
    "    else:\n",
    "        el_raw[\"AlarmDescription_Eng\"] = pd.NA  # Ensure column exists\n",
    "    # Keep AlarmNumber from translation if merge was successful, or use original AlarmID\n",
    "    if (\n",
    "        \"AlarmNumber\" not in el_raw.columns and \"AlarmID\" in el_raw.columns\n",
    "    ):  # If merge didn't add AlarmNumber, it implies AlarmID is the primary numeric ID we have.\n",
    "        el_raw[\"AlarmNumber\"] = el_raw[\n",
    "            \"AlarmID\"\n",
    "        ]  # Use AlarmID as the numeric representation if AlarmNumber didn't come from merge\n",
    "    elif \"AlarmNumber\" not in el_raw.columns:  # If neither, create it as NA\n",
    "        el_raw[\"AlarmNumber\"] = pd.NA\n",
    "\n",
    "# Option 2: Fallback or alternative - map ErrorLog.AlarmID to AlarmTranslation.AlarmCode\n",
    "elif (\n",
    "    \"AlarmID\" in el_raw.columns\n",
    "    and \"AlarmCode\" in alarm_translation_temp.columns\n",
    "    and \"Engels\" in alarm_translation_temp.columns\n",
    "):\n",
    "    print(\n",
    "        \"  Attempting to merge ErrorLog (on AlarmID) with AlarmTranslation (on AlarmCode as fallback)...\"\n",
    "    )\n",
    "    translations_subset_ac = alarm_translation_temp[\n",
    "        [\"AlarmCode\", \"Engels\", \"AlarmNumber\"]\n",
    "    ].drop_duplicates(subset=[\"AlarmCode\"])\n",
    "    el_raw = pd.merge(\n",
    "        el_raw,\n",
    "        translations_subset_ac,\n",
    "        left_on=\"AlarmID\",  # Key from ErrorLog\n",
    "        right_on=\"AlarmCode\",  # Key from AlarmTranslation\n",
    "        how=\"left\",\n",
    "    )\n",
    "    if \"Engels\" in el_raw.columns:\n",
    "        el_raw.rename(columns={\"Engels\": \"AlarmDescription_Eng\"}, inplace=True)\n",
    "    else:\n",
    "        el_raw[\"AlarmDescription_Eng\"] = pd.NA\n",
    "    # Ensure AlarmNumber exists, preferring the one from translation if available\n",
    "    if \"AlarmNumber\" not in el_raw.columns and \"AlarmID\" in el_raw.columns:\n",
    "        el_raw[\"AlarmNumber\"] = el_raw[\"AlarmID\"]\n",
    "    elif \"AlarmNumber\" not in el_raw.columns:\n",
    "        el_raw[\"AlarmNumber\"] = pd.NA\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"ERROR: Cannot determine correct keys to merge ErrorLog with AlarmTranslation for descriptions.\"\n",
    "    )\n",
    "    if \"AlarmDescription_Eng\" not in el_raw.columns:\n",
    "        el_raw[\"AlarmDescription_Eng\"] = pd.NA\n",
    "    if \"AlarmNumber\" not in el_raw.columns:\n",
    "        el_raw[\"AlarmNumber\"] = (\n",
    "            el_raw[\"AlarmID\"] if \"AlarmID\" in el_raw.columns else pd.NA\n",
    "        )\n",
    "\n",
    "# Rename TimeStamp in ErrorLog to avoid conflict\n",
    "if \"TimeStamp\" in el_raw.columns:\n",
    "    el_raw.rename(columns={\"TimeStamp\": \"ErrorLogRecordTimeStamp\"}, inplace=True)\n",
    "\n",
    "print(\n",
    "    f\"  ErrorLog processed. el_raw shape: {el_raw.shape}. Columns include: {el_raw.columns.to_list()}\"\n",
    ")\n",
    "\n",
    "# --- 1.2 Convert epochs / numeric timestamps to datetime objects ---\n",
    "print(\"\\n  Converting timestamp columns to datetime objects...\")\n",
    "for df_temp, name in [\n",
    "    (df_raw, \"df_raw\"),\n",
    "    (df_ext_raw, \"df_ext_raw\"),\n",
    "    (ahu_raw, \"ahu_raw\"),\n",
    "]:\n",
    "    if \"TimeStamp\" in df_temp.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_temp[\"TimeStamp\"]):\n",
    "            df_temp[\"TimeStamp_epoch\"] = df_temp[\"TimeStamp\"]\n",
    "            df_temp[\"TimeStamp_dt\"] = pd.to_datetime(\n",
    "                df_temp[\"TimeStamp\"], unit=\"s\", errors=\"coerce\"\n",
    "            )\n",
    "            if df_temp[\"TimeStamp_dt\"].isnull().any():\n",
    "                print(\n",
    "                    f\"      Warning: NaNs produced during TimeStamp conversion for {name}.\"\n",
    "                )\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df_temp[\"TimeStamp\"]):\n",
    "            print(\n",
    "                f\"    {name}['TimeStamp'] is already datetime. Creating TimeStamp_dt and TimeStamp_epoch.\"\n",
    "            )\n",
    "            df_temp[\"TimeStamp_dt\"] = df_temp[\"TimeStamp\"]\n",
    "            df_temp[\"TimeStamp_epoch\"] = (\n",
    "                df_temp[\"TimeStamp\"] - pd.Timestamp(\"1970-01-01\")\n",
    "            ) // pd.Timedelta(\"1s\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"    Warning: {name}['TimeStamp'] is not numeric or datetime. Cannot convert reliably.\"\n",
    "            )\n",
    "            df_temp[\"TimeStamp_dt\"] = pd.NaT\n",
    "            df_temp[\"TimeStamp_epoch\"] = pd.NA\n",
    "    else:\n",
    "        print(f\"    Warning: 'TimeStamp' column not found in {name}.\")\n",
    "\n",
    "for col_name_numeric, col_name_dt in [\n",
    "    (\"StartTimeStamp\", \"ErrorStartTime_dt\"),\n",
    "    (\"EndTimeStamp\", \"ErrorEndTime_dt\"),\n",
    "]:\n",
    "    if col_name_numeric in el_raw.columns:\n",
    "        if pd.api.types.is_numeric_dtype(el_raw[col_name_numeric]):\n",
    "            el_raw[col_name_dt] = pd.to_datetime(\n",
    "                el_raw[col_name_numeric], unit=\"s\", errors=\"coerce\"\n",
    "            )\n",
    "            if el_raw[col_name_dt].isnull().any():\n",
    "                print(f\"      Warning: NaNs produced for {col_name_dt}.\")\n",
    "        elif pd.api.types.is_datetime64_any_dtype(\n",
    "            el_raw[col_name_numeric]\n",
    "        ):  # If already datetime\n",
    "            el_raw[col_name_dt] = el_raw[col_name_numeric]\n",
    "        else:\n",
    "            print(\n",
    "                f\"    Warning: el_raw['{col_name_numeric}'] is not numeric or datetime.\"\n",
    "            )\n",
    "            el_raw[col_name_dt] = pd.NaT\n",
    "    else:\n",
    "        print(f\"    Warning: '{col_name_numeric}' column not found in el_raw.\")\n",
    "\n",
    "# For mc_raw (MachineCycle)\n",
    "if \"StartDateTime\" in mc_raw.columns:\n",
    "    if pd.api.types.is_numeric_dtype(mc_raw[\"StartDateTime\"]):\n",
    "        mc_raw[\"StartDateTime_epoch\"] = mc_raw[\"StartDateTime\"]\n",
    "        mc_raw[\"StartDateTime_dt\"] = pd.to_datetime(\n",
    "            mc_raw[\"StartDateTime\"], unit=\"s\", errors=\"coerce\"\n",
    "        )\n",
    "        if mc_raw[\"StartDateTime_dt\"].isnull().any():\n",
    "            print(\"      Warning: NaNs produced for StartDateTime_dt.\")\n",
    "    elif pd.api.types.is_datetime64_any_dtype(mc_raw[\"StartDateTime\"]):\n",
    "        mc_raw[\"StartDateTime_dt\"] = mc_raw[\"StartDateTime\"]\n",
    "        mc_raw[\"StartDateTime_epoch\"] = (\n",
    "            mc_raw[\"StartDateTime_dt\"] - pd.Timestamp(\"1970-01-01\")\n",
    "        ) // pd.Timedelta(\"1s\")\n",
    "    else:\n",
    "        print(\"    Warning: mc_raw['StartDateTime'] is not numeric or datetime.\")\n",
    "        mc_raw[\"StartDateTime_dt\"] = pd.NaT\n",
    "        mc_raw[\"StartDateTime_epoch\"] = pd.NA\n",
    "else:\n",
    "    print(\"    Warning: 'StartDateTime' column not found in mc_raw.\")\n",
    "\n",
    "if \"EndDateTime\" in mc_raw.columns:\n",
    "    if pd.api.types.is_numeric_dtype(mc_raw[\"EndDateTime\"]):\n",
    "        mc_raw[\"EndDateTime_epoch_raw\"] = mc_raw[\"EndDateTime\"]\n",
    "        mc_raw[\"EndDateTime_numeric_for_dt\"] = mc_raw[\"EndDateTime\"].replace(0, np.nan)\n",
    "        mc_raw[\"EndDateTime_dt\"] = pd.to_datetime(\n",
    "            mc_raw[\"EndDateTime_numeric_for_dt\"], unit=\"s\", errors=\"coerce\"\n",
    "        )\n",
    "        if mc_raw[\"EndDateTime_dt\"].isnull().any():\n",
    "            print(\"      Warning: NaNs produced for EndDateTime_dt.\")\n",
    "    elif pd.api.types.is_datetime64_any_dtype(\n",
    "        mc_raw[\"EndDateTime\"]\n",
    "    ):  # If it's already datetime (less likely if 0s are present)\n",
    "        mc_raw[\"EndDateTime_dt\"] = mc_raw[\"EndDateTime\"]\n",
    "        mc_raw[\"EndDateTime_epoch_raw\"] = (\n",
    "            mc_raw[\"EndDateTime_dt\"] - pd.Timestamp(\"1970-01-01\")\n",
    "        ) // pd.Timedelta(\"1s\")  # This will be NaT for original 0s if not handled\n",
    "    else:\n",
    "        print(\"    Warning: mc_raw['EndDateTime'] is not numeric or datetime.\")\n",
    "        mc_raw[\"EndDateTime_dt\"] = pd.NaT\n",
    "        mc_raw[\"EndDateTime_epoch_raw\"] = pd.NA\n",
    "else:\n",
    "    print(\"    Warning: 'EndDateTime' column not found in mc_raw.\")\n",
    "\n",
    "print(\"\\n✅ Step 1: Load Raw Data & Type Conversion Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 1.1: Inspecting Columns of Loaded Raw DataFrames ---\")\n",
    "\n",
    "dataframes_to_inspect = {\n",
    "    \"df_raw\": df_raw if \"df_raw\" in locals() else None,\n",
    "    \"df_ext_raw\": df_ext_raw if \"df_ext_raw\" in locals() else None,\n",
    "    \"ahu_raw\": ahu_raw if \"ahu_raw\" in locals() else None,\n",
    "    \"mc_raw\": mc_raw if \"mc_raw\" in locals() else None,\n",
    "    \"mp_raw\": mp_raw if \"mp_raw\" in locals() else None,\n",
    "    \"el_raw\": el_raw if \"el_raw\" in locals() else None,\n",
    "    \"alarm_translation_temp\": alarm_translation_temp\n",
    "    if \"alarm_translation_temp\" in locals()\n",
    "    else None,  # Also inspect this\n",
    "}\n",
    "\n",
    "for name, df_instance in dataframes_to_inspect.items():\n",
    "    print(f\"\\nColumns for DataFrame: {name}\")\n",
    "    if df_instance is not None and not df_instance.empty:\n",
    "        print(df_instance.columns.tolist())\n",
    "        print(f\"Shape: {df_instance.shape}\")\n",
    "        # Print a few dtypes\n",
    "        if len(df_instance.columns) > 0:\n",
    "            print(\"Sample dtypes:\")\n",
    "            print(df_instance.head(1).dtypes.to_string())\n",
    "\n",
    "    elif df_instance is not None and df_instance.empty:\n",
    "        print(\"  DataFrame is empty.\")\n",
    "    else:\n",
    "        print(\"  DataFrame not loaded or not defined in locals().\")\n",
    "\n",
    "print(\"\\n--- End of Column Inspection ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.3 Identify Setter Machines ---\n",
    "print(\"\\n🔍 Step 1.2: Identifying Setter Machines...\")\n",
    "\n",
    "# Check if mp_raw exists and has required columns\n",
    "if (\n",
    "    \"mp_raw\" in locals()\n",
    "    and mp_raw is not None\n",
    "    and \"text\" in mp_raw.columns\n",
    "    and \"MachineID\" in mp_raw.columns\n",
    "):\n",
    "    # Setters have IDs starting with 'S' in MachinePositions\n",
    "    setter_ids = mp_raw.loc[\n",
    "        mp_raw.text.str.startswith(\"S\", na=False), \"MachineID\"\n",
    "    ].unique()\n",
    "    print(f\"✅ Found {len(setter_ids)} setter machines\")\n",
    "    print(\n",
    "        f\"   Setter IDs: {sorted(setter_ids)[:10]}...\"\n",
    "        if len(setter_ids) > 10\n",
    "        else f\"   Setter IDs: {sorted(setter_ids)}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"ERROR: Cannot identify setters. mp_raw not loaded or missing required columns.\"\n",
    "    )\n",
    "    setter_ids = []\n",
    "\n",
    "# --- 1.4 Filter to Setters Only ---\n",
    "if len(setter_ids) > 0:\n",
    "    print(\"\\n🔽 Step 1.3: Filtering all data to setter machines only...\")\n",
    "\n",
    "    # Create filtered datasets with existence checks\n",
    "    df_setters = (\n",
    "        df_raw[df_raw.MachineID.isin(setter_ids)].copy()\n",
    "        if \"df_raw\" in locals() and \"MachineID\" in df_raw.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    df_ext_setters = (\n",
    "        df_ext_raw[df_ext_raw.MachineID.isin(setter_ids)].copy()\n",
    "        if \"df_ext_raw\" in locals() and \"MachineID\" in df_ext_raw.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    ahu_setters = (\n",
    "        ahu_raw[ahu_raw.MachineID.isin(setter_ids)].copy()\n",
    "        if \"ahu_raw\" in locals() and \"MachineID\" in ahu_raw.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    mc_setters = (\n",
    "        mc_raw[mc_raw.MachineID.isin(setter_ids)].copy()\n",
    "        if \"mc_raw\" in locals() and \"MachineID\" in mc_raw.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    el_setters = (\n",
    "        el_raw[el_raw.MachineID.isin(setter_ids)].copy()\n",
    "        if \"el_raw\" in locals() and \"MachineID\" in el_raw.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "    # Summary of filtered data\n",
    "    print(\"\\n📊 Filtered Data Summary:\")\n",
    "    if not df_setters.empty:\n",
    "        print(f\"  - ClimateLog setters: {df_setters.shape} (from {df_raw.shape})\")\n",
    "    if not df_ext_setters.empty:\n",
    "        print(\n",
    "            f\"  - ClimateLogExtra setters: {df_ext_setters.shape} (from {df_ext_raw.shape})\"\n",
    "        )\n",
    "    if not ahu_setters.empty:\n",
    "        print(f\"  - AHUClimateLog setters: {ahu_setters.shape} (from {ahu_raw.shape})\")\n",
    "    if not mc_setters.empty:\n",
    "        print(f\"  - MachineCycle setters: {mc_setters.shape} (from {mc_raw.shape})\")\n",
    "    if not el_setters.empty:\n",
    "        print(f\"  - ErrorLog setters: {el_setters.shape} (from {el_raw.shape})\")\n",
    "\n",
    "    print(\"\\n✅ Phase 1: Data Ingestion & Setter Filtering Complete!\")\n",
    "else:\n",
    "    print(\"ERROR: No setter machines found. Cannot proceed with filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Enhanced EDA Helper Functions\n",
    "def perform_detailed_eda(df, table_name, sample_size=None):\n",
    "    \"\"\"Perform detailed exploratory data analysis on a dataframe\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"📊 Detailed EDA for {table_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Sample if requested\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        print(\n",
    "            f\"Note: Sampling {sample_size} rows from {len(df)} total rows for analysis\"\n",
    "        )\n",
    "    else:\n",
    "        df_sample = df\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(\"\\nMemory Usage:\")\n",
    "    print(df.memory_usage(deep=True).sort_values(ascending=False).head(10))\n",
    "\n",
    "    # Column types summary\n",
    "    print(\"\\nColumn Type Summary:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame(\n",
    "        {\"Missing_Count\": missing, \"Missing_Percentage\": missing_pct}\n",
    "    ).sort_values(\"Missing_Count\", ascending=False)\n",
    "\n",
    "    if missing_df[\"Missing_Count\"].sum() > 0:\n",
    "        print(\"\\nMissing Values (Top 10):\")\n",
    "        print(missing_df[missing_df[\"Missing_Count\"] > 0].head(10))\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "\n",
    "    # Basic statistics for numeric columns\n",
    "    numeric_cols = df_sample.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\nNumeric Column Statistics (first 10 columns):\")\n",
    "        print(df_sample[numeric_cols[:10]].describe().round(2))\n",
    "\n",
    "    # Unique value counts for categorical columns\n",
    "    categorical_cols = df_sample.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\nCategorical Column Unique Values:\")\n",
    "        for col in categorical_cols[:5]:  # First 5 categorical columns\n",
    "            unique_count = df_sample[col].nunique()\n",
    "            print(f\"  - {col}: {unique_count} unique values\")\n",
    "            if unique_count <= 10:\n",
    "                print(f\"    Values: {df_sample[col].value_counts().to_dict()}\")\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "\n",
    "# Test EDA on a sample\n",
    "if \"df_setters\" in locals() and not df_setters.empty:\n",
    "    perform_detailed_eda(df_setters, \"ClimateLog (Setters)\", sample_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting Phase 2: Cycle Splits & Alarm Detection\")\n",
    "\n",
    "# --- Configuration for Critical Alarms ---\n",
    "# Using the comprehensive list from your code\n",
    "CRITICAL_ALARM_NAMES = [\n",
    "    \"Connection lost error\",\n",
    "    \"RS422 connection lost\",\n",
    "    \"Clocktime lost error\",\n",
    "    \"RTC slow error\",\n",
    "    \"sensor error\",\n",
    "    \"diagnose error\",\n",
    "    \"Temperature high error\",\n",
    "    \"Temperature low error\",\n",
    "    \"Fan alarm\",\n",
    "    \"Power failure\",\n",
    "    \"Incubation program lost\",\n",
    "    \"Emergency stop\",\n",
    "    \"setpoints lost\",\n",
    "    \"stack overflow\",\n",
    "    \"program lost warning\",\n",
    "    \"user int error\",\n",
    "    \"can bus warning\",\n",
    "]\n",
    "print(\n",
    "    f\"📋 Using {len(CRITICAL_ALARM_NAMES)} defined critical alarm names for filtering.\"\n",
    ")\n",
    "\n",
    "# Additional critical alarms from the original plan (if they exist in your data)\n",
    "# You can add: \"temperatuur hoog\", \"temperature decrease\", \"cooling failure\"\n",
    "\n",
    "# Check prerequisites\n",
    "if \"mc_setters\" not in locals() or mc_setters.empty:\n",
    "    print(\n",
    "        \"❌ Error: 'mc_setters' is not available or empty. Cannot proceed with Phase 2.\"\n",
    "    )\n",
    "    mc_clean = pd.DataFrame()\n",
    "    mc_alarm = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"✅ Input mc_setters shape: {mc_setters.shape}\")\n",
    "\n",
    "    # --- 2.1 Filter for Completed Setter Runs ---\n",
    "    mc_temp = mc_setters.copy()\n",
    "\n",
    "    # Filter for CycleType == 0 (normal setter cycles)\n",
    "    if \"CycleType\" in mc_temp.columns:\n",
    "        mc_temp = mc_temp[mc_temp[\"CycleType\"] == 0]\n",
    "        print(f\"   Filtered to CycleType==0: {mc_temp.shape[0]} cycles\")\n",
    "    else:\n",
    "        print(\"   ⚠️ Warning: 'CycleType' column not found in mc_setters.\")\n",
    "\n",
    "    # Filter for completed cycles (EndDateTime exists)\n",
    "    if \"EndDateTime_dt\" in mc_temp.columns:\n",
    "        mc_temp = mc_temp[mc_temp[\"EndDateTime_dt\"].notna()]\n",
    "        print(f\"   Filtered to completed cycles: {mc_temp.shape[0]} cycles\")\n",
    "    else:\n",
    "        print(\n",
    "            \"   ❌ Warning: 'EndDateTime_dt' column not found. Cannot filter for completed cycles.\"\n",
    "        )\n",
    "        mc_temp = pd.DataFrame()\n",
    "\n",
    "    # Calculate cycle duration\n",
    "    if (\n",
    "        not mc_temp.empty\n",
    "        and \"StartDateTime_dt\" in mc_temp.columns\n",
    "        and \"EndDateTime_dt\" in mc_temp.columns\n",
    "    ):\n",
    "        mc_temp[\"duration_days\"] = (\n",
    "            mc_temp[\"EndDateTime_dt\"] - mc_temp[\"StartDateTime_dt\"]\n",
    "        ).dt.total_seconds() / 86400\n",
    "        print(f\"   ✅ Calculated duration for {mc_temp.shape[0]} cycles\")\n",
    "\n",
    "        # Show duration statistics\n",
    "        print(\"\\n   Duration Statistics (all completed cycles):\")\n",
    "        print(f\"     Mean: {mc_temp['duration_days'].mean():.2f} days\")\n",
    "        print(f\"     Std:  {mc_temp['duration_days'].std():.2f} days\")\n",
    "        print(f\"     Min:  {mc_temp['duration_days'].min():.2f} days\")\n",
    "        print(f\"     Max:  {mc_temp['duration_days'].max():.2f} days\")\n",
    "    else:\n",
    "        print(\n",
    "            \"   ⚠️ Warning: Could not calculate duration due to missing datetime columns.\"\n",
    "        )\n",
    "        mc_temp[\"duration_days\"] = np.nan\n",
    "\n",
    "    mc_completed_setters_all_durations = mc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2 Identify CycleIDs affected by critical alarms ---\n",
    "alarm_cycle_ids = set()\n",
    "\n",
    "if \"el_setters\" not in locals() or el_setters.empty:\n",
    "    print(\"⚠️ Warning: 'el_setters' (ErrorLog for setters) is not available or empty.\")\n",
    "    print(\"   Assuming no cycles have critical alarms.\")\n",
    "elif not all(\n",
    "    col in el_setters.columns\n",
    "    for col in [\"MachineID\", \"ErrorStartTime_dt\", \"AlarmDescription_Eng\"]\n",
    "):\n",
    "    print(\"⚠️ Warning: 'el_setters' is missing required columns.\")\n",
    "    print(f\"   Available columns: {el_setters.columns.tolist()}\")\n",
    "elif not mc_completed_setters_all_durations.empty:\n",
    "    # Filter for critical alarms\n",
    "    critical_error_log = el_setters[\n",
    "        el_setters[\"AlarmDescription_Eng\"].isin(CRITICAL_ALARM_NAMES)\n",
    "    ]\n",
    "    print(f\"\\n🚨 Found {len(critical_error_log)} critical error log entries\")\n",
    "\n",
    "    if len(critical_error_log) > 0:\n",
    "        # Show distribution of critical alarms\n",
    "        print(\"\\n   Top 10 Critical Alarms:\")\n",
    "        alarm_counts = (\n",
    "            critical_error_log[\"AlarmDescription_Eng\"].value_counts().head(10)\n",
    "        )\n",
    "        for alarm, count in alarm_counts.items():\n",
    "            print(f\"     - {alarm}: {count}\")\n",
    "\n",
    "    if not critical_error_log.empty:\n",
    "        print(\"\\n🔍 Checking for critical alarms overlapping with cycle durations...\")\n",
    "        print(f\"   Processing {len(mc_completed_setters_all_durations)} cycles...\")\n",
    "\n",
    "        # Progress tracking\n",
    "        cycles_processed = 0\n",
    "\n",
    "        # Iterate through cycles to find overlaps\n",
    "        for _, cycle_row in mc_completed_setters_all_durations.iterrows():\n",
    "            machine_id = cycle_row[\"MachineID\"]\n",
    "            cycle_id = cycle_row[\"CycleID\"]\n",
    "            c_start = cycle_row[\"StartDateTime_dt\"]\n",
    "            c_end = cycle_row[\"EndDateTime_dt\"]\n",
    "\n",
    "            if pd.isna(c_start) or pd.isna(c_end):\n",
    "                continue\n",
    "\n",
    "            # Filter errors for the same machine\n",
    "            machine_critical_errors = critical_error_log[\n",
    "                critical_error_log[\"MachineID\"] == machine_id\n",
    "            ]\n",
    "\n",
    "            # Check if any error overlaps with this cycle\n",
    "            for _, error_row in machine_critical_errors.iterrows():\n",
    "                e_start = error_row[\"ErrorStartTime_dt\"]\n",
    "                if pd.isna(e_start):\n",
    "                    continue\n",
    "\n",
    "                # Check if error start time is within the cycle duration\n",
    "                if c_start <= e_start <= c_end:\n",
    "                    alarm_cycle_ids.add(cycle_id)\n",
    "                    break  # Found one critical error, no need to check more\n",
    "\n",
    "            cycles_processed += 1\n",
    "            if cycles_processed % 500 == 0:\n",
    "                print(\n",
    "                    f\"     Processed {cycles_processed}/{len(mc_completed_setters_all_durations)} cycles...\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\n   ✅ Finished processing {cycles_processed} cycles\")\n",
    "        print(f\"   🚨 Identified {len(alarm_cycle_ids)} cycles with critical alarms\")\n",
    "    else:\n",
    "        print(\"   ℹ️ No critical error log entries found matching CRITICAL_ALARM_NAMES.\")\n",
    "else:\n",
    "    print(\"   ℹ️ No completed setter cycles to check for alarms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.3 Split into mc_clean and mc_alarm ---\n",
    "if not mc_completed_setters_all_durations.empty:\n",
    "    # Split based on alarm presence\n",
    "    mc_alarm_temp = mc_completed_setters_all_durations[\n",
    "        mc_completed_setters_all_durations[\"CycleID\"].isin(list(alarm_cycle_ids))\n",
    "    ].copy()\n",
    "\n",
    "    mc_clean_temp = mc_completed_setters_all_durations[\n",
    "        ~mc_completed_setters_all_durations[\"CycleID\"].isin(list(alarm_cycle_ids))\n",
    "    ].copy()\n",
    "\n",
    "    print(\"\\n📊 Initial split:\")\n",
    "    print(f\"   - Cycles with alarms: {mc_alarm_temp.shape[0]}\")\n",
    "    print(f\"   - Clean cycles: {mc_clean_temp.shape[0]}\")\n",
    "else:\n",
    "    mc_alarm_temp = pd.DataFrame()\n",
    "    mc_clean_temp = pd.DataFrame()\n",
    "\n",
    "# --- 2.4 Apply 18-21 day duration filter ---\n",
    "duration_min_days = 18\n",
    "duration_max_days = 21\n",
    "\n",
    "print(\n",
    "    f\"\\n🔽 Applying duration filter ({duration_min_days}-{duration_max_days} days)...\"\n",
    ")\n",
    "\n",
    "# Filter clean cycles\n",
    "if not mc_clean_temp.empty and \"duration_days\" in mc_clean_temp.columns:\n",
    "    mc_clean = mc_clean_temp[\n",
    "        (mc_clean_temp[\"duration_days\"] >= duration_min_days)\n",
    "        & (mc_clean_temp[\"duration_days\"] <= duration_max_days)\n",
    "    ].copy()\n",
    "    dropped_clean = len(mc_clean_temp) - len(mc_clean)\n",
    "    print(\n",
    "        f\"   ✅ mc_clean: {len(mc_clean)} cycles (dropped {dropped_clean} outside duration range)\"\n",
    "    )\n",
    "else:\n",
    "    mc_clean = pd.DataFrame()\n",
    "    print(\"   ⚠️ mc_clean_temp is empty or missing 'duration_days'\")\n",
    "\n",
    "# Filter alarm cycles\n",
    "if not mc_alarm_temp.empty and \"duration_days\" in mc_alarm_temp.columns:\n",
    "    mc_alarm = mc_alarm_temp[\n",
    "        (mc_alarm_temp[\"duration_days\"] >= duration_min_days)\n",
    "        & (mc_alarm_temp[\"duration_days\"] <= duration_max_days)\n",
    "    ].copy()\n",
    "    dropped_alarm = len(mc_alarm_temp) - len(mc_alarm)\n",
    "    print(\n",
    "        f\"   ✅ mc_alarm: {len(mc_alarm)} cycles (dropped {dropped_alarm} outside duration range)\"\n",
    "    )\n",
    "else:\n",
    "    mc_alarm = pd.DataFrame()\n",
    "    print(\"   ⚠️ mc_alarm_temp is empty or missing 'duration_days'\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n📊 Final Phase 2 Results:\")\n",
    "print(\n",
    "    f\"   - Clean cycles (18-21 days): {mc_clean.shape if 'mc_clean' in locals() and not mc_clean.empty else '0 cycles'}\"\n",
    ")\n",
    "print(\n",
    "    f\"   - Alarm cycles (18-21 days): {mc_alarm.shape if 'mc_alarm' in locals() and not mc_alarm.empty else '0 cycles'}\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Phase 2: Cycle Splits & Alarm Detection Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing Cycle Durations ---\n",
    "print(\"\\n📊 Visualizing Cycle Durations...\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot for mc_clean ---\n",
    "if (\n",
    "    \"mc_clean\" in locals()\n",
    "    and not mc_clean.empty\n",
    "    and \"duration_days\" in mc_clean.columns\n",
    "):\n",
    "    print(\"\\n✅ Clean Cycles Duration Statistics:\")\n",
    "    print(mc_clean[\"duration_days\"].describe().round(2))\n",
    "\n",
    "    ax1 = axes[0]\n",
    "    mc_clean[\"duration_days\"].plot(\n",
    "        kind=\"hist\", bins=30, edgecolor=\"black\", alpha=0.7, color=\"green\", ax=ax1\n",
    "    )\n",
    "    ax1.set_title(\"Histogram of Cycle Durations for Clean Cycles\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Cycle Duration (days)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Number of Cycles\", fontsize=12)\n",
    "    ax1.axvline(18, color=\"red\", linestyle=\"--\", linewidth=2, label=\"18 days (min)\")\n",
    "    ax1.axvline(21, color=\"red\", linestyle=\"--\", linewidth=2, label=\"21 days (max)\")\n",
    "    ax1.axvline(\n",
    "        mc_clean[\"duration_days\"].mean(),\n",
    "        color=\"blue\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean: {mc_clean['duration_days'].mean():.2f}\",\n",
    "    )\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle=\":\", alpha=0.7)\n",
    "else:\n",
    "    axes[0].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No clean cycles data available\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[0].set_title(\"Clean Cycles - No Data\")\n",
    "\n",
    "# --- Plot for mc_alarm ---\n",
    "if (\n",
    "    \"mc_alarm\" in locals()\n",
    "    and not mc_alarm.empty\n",
    "    and \"duration_days\" in mc_alarm.columns\n",
    "):\n",
    "    print(\"\\n🚨 Alarm Cycles Duration Statistics:\")\n",
    "    print(mc_alarm[\"duration_days\"].describe().round(2))\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    mc_alarm[\"duration_days\"].plot(\n",
    "        kind=\"hist\", bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\", ax=ax2\n",
    "    )\n",
    "    ax2.set_title(\"Histogram of Cycle Durations for Alarm Cycles\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Cycle Duration (days)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Number of Cycles\", fontsize=12)\n",
    "    ax2.axvline(18, color=\"darkred\", linestyle=\"--\", linewidth=2, label=\"18 days (min)\")\n",
    "    ax2.axvline(21, color=\"darkred\", linestyle=\"--\", linewidth=2, label=\"21 days (max)\")\n",
    "    ax2.axvline(\n",
    "        mc_alarm[\"duration_days\"].mean(),\n",
    "        color=\"blue\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean: {mc_alarm['duration_days'].mean():.2f}\",\n",
    "    )\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle=\":\", alpha=0.7)\n",
    "else:\n",
    "    axes[1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No alarm cycles data available\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[1].set_title(\"Alarm Cycles - No Data\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "if (\n",
    "    \"mc_clean\" in locals()\n",
    "    and not mc_clean.empty\n",
    "    and \"mc_alarm\" in locals()\n",
    "    and not mc_alarm.empty\n",
    "):\n",
    "    print(\"\\n📊 Duration Comparison:\")\n",
    "    print(f\"   Clean cycles mean duration: {mc_clean['duration_days'].mean():.2f} days\")\n",
    "    print(f\"   Alarm cycles mean duration: {mc_alarm['duration_days'].mean():.2f} days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
